\subsection{The Accent Gap in Automatic Speech Recognition}

Modern ASR systems exhibit systematic performance degradation on non-standard English varieties---a disparity with serious implications for equitable access to voice-enabled services. Koenecke et al. \cite{koenecke2020} conducted the seminal quantitative study, evaluating five major commercial ASR systems across racial demographics. Their findings were stark: word error rates averaged 0.35 for Black speakers compared to 0.19 for White speakers, with 23\% of Black speaker audio producing WER exceeding 0.50---functionally unusable transcription---compared to just 1.6\% for White speakers. Critically, the researchers traced these disparities to acoustic models rather than language models, as the performance gap persisted even on identical phrases.

The Edinburgh International Accents of English Corpus (EdAcc) benchmark extends this analysis to global accent variation \cite{sanabria2023}. Testing revealed that OpenAI's Whisper-large model achieved 19.7\% WER on EdAcc compared to just 2.7\% on LibriSpeech test-clean---a seven-fold performance degradation on accented speech. The study specifically identified Jamaican English among the accents with highest error rates, directly validating concerns about Caribbean speech recognition.

Research on African-accented English provides methodologically rigorous comparators. The AfriSpeech-200 corpus encompasses 200 hours of Pan-African English speech across 120 accents from 13 Anglophone countries, with evaluations demonstrating that models achieving 1-3\% WER on standard corpora produce 10-90\% WER on African-accented subsets \cite{olatunji2023}. Named entities and domain-specific terminology proved particularly challenging---a finding directly relevant to emergency contexts where accurate location and hazard extraction is critical.

Caribbean English remains especially underserved despite representing millions of speakers. Madden et al. \cite{madden2025} developed the first substantial Jamaican Patois speech corpus (42.58 hours) and derived scaling laws for Whisper performance on this variety. Their results are instructive: pre-trained Whisper Large achieved 89\% WER on Patois---functionally useless---while fine-tuned Whisper Medium reduced this to 30\% WER. Notably, fine-tuned Whisper Tiny outperformed non-fine-tuned Whisper Large, demonstrating that domain-specific data matters more than model size for underrepresented varieties. Their scaling law (WER = 158.06 $\times$ M$^{-0.255}$ $\times$ D$^{-0.269}$) reveals that dataset increases yield greater gains than model scaling for this population, informing our choice of Whisper Medium with Caribbean-specific fine-tuning.
